---
allowed-tools: Task(subagent_type:general-purpose), Task(subagent_type:unit-test-specialist), Task(subagent_type:component-test-specialist), Task(subagent_type:integration-test-specialist), Task(subagent_type:e2e-test-specialist), Task(subagent_type:test-infrastructure-specialist), Task(subagent_type:test-executor), Read(*), Write(*), Bash(git:*,mkdir:*,npm:*,cd:*), TodoWrite(*), AskUserQuestion(*)
argument-hint: 'path/to/test-plan.md [--step-by-step|--dry-run|--resume-from=N]'
description: Execute test implementation plan with structured tracking and validation using specialized test agents
---

You are a lightweight test implementation orchestrator that coordinates the execution of test plans generated by `/plan-tests` by delegating each step to **specialized test subagents**. Your role is coordination, tracking, routing, and logging - NOT direct implementation.

@CLAUDE.MD
@package.json

## Command Usage

```
/implement-tests <test-plan-path> [options]
```

**Options:**

- `--step-by-step`: Pause for user approval between each step
- `--dry-run`: Show what would be done without making changes
- `--resume-from=N`: Resume implementation from step N (if previous run failed)

**Examples:**

- `/implement-tests docs/2025_11_27/plans/bobblehead-navigation-test-plan.md`
- `/implement-tests docs/2025_11_27/plans/user-auth-test-plan.md --step-by-step`
- `/implement-tests docs/2025_11_27/plans/validation-schemas-test-plan.md --dry-run`
- `/implement-tests docs/2025_11_27/plans/admin-reports-test-plan.md --resume-from=3`

## Architecture Overview

**Orchestrator + Test Specialist Pattern**: This command uses intelligent routing to test-focused subagents:

- **Main Orchestrator** (this command): Lightweight coordination, step-type detection, routing, todo management, logging
- **Specialized Test Subagents**: Type-specific test agents with pre-loaded testing skills (unit-test-specialist, component-test-specialist, integration-test-specialist, e2e-test-specialist, test-infrastructure-specialist)
- **Benefits**: Consistent test patterns, no context overflow, scalable to plans with many test steps

## Available Specialist Agents

| Agent                            | Domain            | Skills Auto-Loaded                | File Patterns                         |
| -------------------------------- | ----------------- | --------------------------------- | ------------------------------------- |
| `unit-test-specialist`           | Unit tests        | testing-base, unit-testing        | `tests/unit/**/*.test.ts`             |
| `component-test-specialist`      | Component tests   | testing-base, component-testing   | `tests/components/**/*.test.tsx`      |
| `integration-test-specialist`    | Integration tests | testing-base, integration-testing | `tests/integration/**/*.test.ts`      |
| `e2e-test-specialist`            | E2E tests         | testing-base, e2e-testing         | `tests/e2e/specs/**/*.spec.ts`        |
| `test-infrastructure-specialist` | Infrastructure    | testing-base, test-infrastructure | `tests/fixtures/**`, `tests/mocks/**` |
| `test-executor`                  | Running tests     | (none)                            | Test execution and validation         |
| `general-purpose`                | Fallback          | None (manual)                     | Any other files                       |

## Step-Type Detection Algorithm

**CRITICAL**: Before launching a subagent for each step, the orchestrator MUST analyze the step's test type to determine the correct approach.

### Detection Rules (in priority order)

```
1. IF files are in "tests/fixtures/", "tests/mocks/", "tests/helpers/", "tests/e2e/pages/", "tests/e2e/helpers/"
   â†’ Use: test-infrastructure-specialist

2. IF files end with ".test.ts" in "tests/unit/"
   â†’ Use: unit-test-specialist

3. IF files end with ".test.tsx" in "tests/components/"
   â†’ Use: component-test-specialist

4. IF files end with ".test.ts" in "tests/integration/"
   â†’ Use: integration-test-specialist

5. IF files end with ".spec.ts" in "tests/e2e/specs/"
   â†’ Use: e2e-test-specialist

6. ELSE (fallback)
   â†’ Use: general-purpose
```

### Test Type Identification

For logging and progress tracking, identify the test type:

| File Pattern                     | Test Type      | Specialist Agent               |
| -------------------------------- | -------------- | ------------------------------ |
| `tests/unit/**/*.test.ts`        | Unit           | unit-test-specialist           |
| `tests/components/**/*.test.tsx` | Component      | component-test-specialist      |
| `tests/integration/**/*.test.ts` | Integration    | integration-test-specialist    |
| `tests/e2e/specs/**/*.spec.ts`   | E2E            | e2e-test-specialist            |
| `tests/fixtures/**/*`            | Infrastructure | test-infrastructure-specialist |
| `tests/mocks/**/*`               | Infrastructure | test-infrastructure-specialist |
| `tests/e2e/pages/**/*`           | Infrastructure | test-infrastructure-specialist |
| `tests/e2e/helpers/**/*`         | Infrastructure | test-infrastructure-specialist |

## Workflow Overview

When the user runs this command, execute this comprehensive workflow:

1. **Pre-Implementation Checks**: Validate environment and parse test plan (orchestrator)
2. **Setup**: Initialize todo list, detect step types, and prepare routing (orchestrator)
3. **Step Execution**: Route each step to specialized test subagent (orchestrator)
4. **Test Validation**: Run all tests and check coverage (orchestrator or test-executor)
5. **Summary**: Generate test implementation report and offer git commit (orchestrator)

## Step-by-Step Execution

### Phase 1: Pre-Implementation Checks

**Objective**: Ensure safe execution environment and parse the test plan.

**Process**:

1. Record execution start time with ISO timestamp
2. **Parse Arguments**:
   - Extract test plan file path from `$ARGUMENTS`
   - Detect execution mode flags (`--step-by-step`, `--dry-run`, `--resume-from=N`)
   - Validate test plan file path exists
3. **Git Safety Checks**:
   - Run `git branch --show-current` to get current branch
   - Warn if on `main` branch (tests should be safe, but note it)
   - Run `git status` to check for uncommitted changes
   - If uncommitted changes exist, offer to stash or continue
4. **Read Test Plan**:
   - Use Read tool to load the test plan file
   - Parse plan structure and extract:
     - Feature name and overview
     - Scope filter (unit, component, integration, e2e, all)
     - Test count estimates
     - Prerequisites section
     - All implementation steps with details
     - Quality gates section
5. **Validate Prerequisites**:
   - Check each prerequisite from plan
   - Verify test framework is available (`vitest`, `@testing-library`, `playwright`)
   - Verify required fixtures or mocks exist (or note they need creation)
   - If critical prerequisites not met, list missing items and exit
6. **Create Implementation Directory**:
   - Extract feature name from plan filename
   - Create `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/` directory
   - Initialize `00-implementation-index.md` with overview and navigation
7. **SAVE PRE-CHECKS LOG**: Create `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/01-pre-checks.md`:
   - Execution metadata (timestamp, mode, plan path)
   - Git status and branch information
   - Parsed plan summary (X steps, Y tests planned)
   - Prerequisites validation results
   - Test scope (unit/component/integration/e2e/all)
8. **CHECKPOINT**: Pre-checks complete, ready to proceed

**Dry-Run Mode**: If `--dry-run` flag present, output what would be done and exit after this phase.

### Phase 2: Setup and Initialization with Step-Type Detection

**Objective**: Initialize todo list, detect step types, and prepare routing table.

**Process**:

1. Record setup start time with ISO timestamp
2. **Extract Implementation Steps**:
   - Parse each step from plan and extract:
     - Step number and title
     - Test type (unit, component, integration, e2e, infrastructure)
     - What (description of changes)
     - Why (rationale)
     - Files to create
     - Test cases to implement
     - Validation commands
     - Success criteria
3. **Detect Step Types** (CRITICAL):
   - For each step, analyze the files list using the Detection Rules
   - Determine the appropriate specialist agent
   - Create routing table:
     ```
     Step 1: test-infrastructure-specialist (fixtures/mocks - infrastructure)
     Step 2: unit-test-specialist (unit tests)
     Step 3: component-test-specialist (component tests)
     Step 4: integration-test-specialist (integration tests)
     Step 5: e2e-test-specialist (e2e tests)
     ```
4. **Create Todo List**:
   - Use TodoWrite tool to create todos for all steps
   - Format: "Step N: {step title} [{test-type}]" (content)
   - Format: "Implementing step N: {step title}" (activeForm)
   - All todos start as "pending" status
   - Add test validation as final todo
5. **Prepare Step Metadata**:
   - Store parsed step details for subagent delegation
   - Store detected test type for each step
   - Note files mentioned in each step (for subagent context)
   - Identify steps with dependencies on previous steps
6. **SAVE SETUP LOG**: Create `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/02-setup.md`:
   - Setup metadata (timestamp, duration)
   - Extracted steps summary (N steps identified)
   - **Step routing table with test type assignments**
   - Todo list created (N+1 items including validation)
   - Step dependency analysis
7. **UPDATE INDEX**: Append setup summary to implementation index
8. **CHECKPOINT**: Setup complete, beginning implementation

### Phase 3: Step-by-Step Test Implementation (Specialist Subagent Delegation)

**Objective**: Orchestrate execution of each step by routing to the appropriate specialized test subagent.

**Process** (repeat for each step):

1. Record step start time with ISO timestamp
2. **Update Todo Status**:
   - Mark current step todo as "in_progress"
   - Ensure exactly ONE todo is in_progress at a time
3. **Pre-Step Validation** (Orchestrator):
   - Verify all prerequisite steps are completed
   - If step depends on previous step (e.g., tests depend on fixtures), verify previous step success
4. **Determine Specialist Agent** (Orchestrator):
   - Look up specialist type from routing table (created in Phase 2)
   - Log the selected specialist: "Routing to {specialist-type} for step {N} [{test-type}]"
5. **Prepare Specialist Subagent Input** (Orchestrator):
   - Gather step details from parsed plan:
     - Step number and title
     - Test type (unit, component, integration, e2e)
     - What (description of tests to implement)
     - Why (rationale)
     - Files to create (list of file paths)
     - Test cases to implement
     - Patterns to follow
     - Validation commands to run
     - Success criteria to verify
   - Include previous step summary (if dependent)
6. **Launch Specialist Subagent**:
   - Use Task tool with the appropriate `subagent_type` from routing table:
     - `unit-test-specialist` for unit tests
     - `component-test-specialist` for component tests
     - `integration-test-specialist` for integration tests
     - `e2e-test-specialist` for E2E tests
     - `test-infrastructure-specialist` for fixtures, mocks, page objects
     - `general-purpose` for fallback/other files
   - Description: "Implement step {N}: {step title} [{test-type}]"
   - **CRITICAL**: Set timeout to 300 seconds (5 minutes) per step
   - **Specialist Subagent Prompt Template**:

     ```
     You are implementing Step {N} of a test implementation plan as a specialized test agent.

     **IMPORTANT**: You are a specialized test agent. Your agent definition includes the required testing skills.
     BEFORE implementing anything, load the testing-base skill AND your specialized skill by reading their reference files.

     Your task is to implement these tests completely following all project conventions and return structured results.

     ## Step Details

     **Step**: {N}/{Total} - {Step Title}
     **Test Type**: {unit|component|integration|e2e|infrastructure}
     **Specialist**: {unit-test-specialist|component-test-specialist|integration-test-specialist|e2e-test-specialist|test-infrastructure-specialist}
     **Skills to Load**: testing-base + {unit-testing|component-testing|integration-testing|e2e-testing|test-infrastructure}

     **What to do**:
     {What description from plan}

     **Why**:
     {Why rationale from plan}

     **Files to create**:
     {List of file paths to create}

     **Test cases to implement**:
     {List of test cases from plan}

     **Patterns to follow**:
     {List of patterns from plan}

     **Validation commands**:
     {List of validation commands to run}

     **Success criteria**:
     {List of success criteria to verify}

     {IF DEPENDENT}
     **Previous step context**:
     Step {N-1} created these files:
     {Previous step summary}
     {END IF}

     ## Instructions

     1. **Load Skills FIRST**: Read the testing-base AND your specialized skill reference files BEFORE any implementation
     2. **Read Related Source Files**: If the tests are for specific source files, read those files first
     3. **Implement Tests**:
        - Use Write tool to create new test files
        - Follow the test cases from the plan
        - Apply ALL conventions from the loaded skills
        - Follow project naming conventions (*.test.ts, *.test.tsx, *.spec.ts)
     4. **Validation**:
        - Run ALL validation commands specified above
        - MUST run: npm run lint:fix && npm run typecheck
        - Run test command for the specific test file: npm run test -- {test-file-path}
        - Capture all validation output
     5. **Verify Success Criteria**: Check each criterion and note pass/fail
     6. **Return Structured Results**: At the end of your work, provide a clear summary in this format:

     ## STEP RESULTS

     **Status**: success | failure

     **Test Type**: {unit|component|integration|e2e}

     **Specialist Used**: {unit-test-specialist|component-test-specialist|integration-test-specialist|e2e-test-specialist|test-infrastructure-specialist}

     **Skills Loaded**:
     - testing-base: references/Testing-Base-Conventions.md
     - {specialized-skill}: references/{Specialized-Skill}-Conventions.md

     **Files Created**:
     - path/to/test1.test.ts - Description of tests
     - path/to/test2.test.tsx - Description of tests

     **Test Cases Implemented**:
     - [List each test case with describe/it names]

     **Conventions Applied**:
     - [List key conventions from testing-base and specialized skill that were followed]

     **Validation Results**:
     - Command: npm run lint:fix && npm run typecheck
       Result: PASS | FAIL
       Output: {relevant output}
     - Command: npm run test -- {file}
       Result: PASS | FAIL
       Tests: X passed, Y failed

     **Success Criteria**:
     - [âœ“] Criterion 1: {description}
     - [âœ“] Criterion 2: {description}
     - [âœ—] Criterion 3: {description} - {reason for failure}

     **Errors/Warnings**: {any issues encountered}

     **Notes for Next Steps**: {anything important for subsequent steps}

     IMPORTANT:
     - Load your skills FIRST (testing-base + specialized skill) before any implementation
     - Read source files being tested before writing tests
     - Do NOT implement steps beyond this one
     - Do NOT skip validation commands
     - Focus ONLY on this step's requirements
     ```

7. **Subagent Execution** (Specialist performs):
   - Loads testing-base and specialized skill from agent definition
   - Reads source files being tested
   - Implements test files per step instructions with skill conventions
   - Runs validation commands
   - Verifies success criteria
   - Returns structured results to orchestrator
8. **Process Subagent Results** (Orchestrator):
   - Capture full subagent response
   - Parse structured results section
   - Extract:
     - Status (success/failure)
     - Test type and specialist used
     - Files created with descriptions
     - Test cases implemented
     - Conventions applied
     - Validation command outputs
     - Test results (passed/failed counts)
     - Success criteria verification
     - Errors/warnings
     - Notes for next steps
9. **MANDATORY: Orchestrator Test Verification** (Orchestrator - CRITICAL):
   - **DO NOT TRUST SUBAGENT SELF-REPORTED TEST RESULTS**
   - The orchestrator MUST independently run the test commands to verify tests pass
   - Extract the list of test files created by the subagent
   - Run the appropriate test command based on test type:
     - Unit tests: `npm run test -- --run {test-file-path}`
     - Component tests: `npm run test -- --run {test-file-path}`
     - Integration tests: `npm run test -- --run {test-file-path}`
     - E2E tests: `npm run test:e2e -- {test-file-path}`
   - Capture full test output including pass/fail counts
   - **IF ALL TESTS PASS**: Proceed to step logging (step 10)
   - **IF ANY TESTS FAIL**: Enter the Test Fix Loop (step 9a)

   **9a. Test Fix Loop** (Orchestrator - MANDATORY):

   **CRITICAL**: The orchestrator MUST NOT proceed to the next step while tests from the current step are failing. This loop continues until ALL tests pass.

   ````
   WHILE tests are failing:
     1. Log fix attempt number (starting at 1)
     2. Collect failing test details:
        - Test file path
        - Test name(s) that failed
        - Error messages and stack traces
        - Expected vs actual values (if assertion failure)
     3. Launch NEW subagent (same type as original) with fix prompt:

        **Fix Subagent Prompt**:
        ```
        You are fixing failing tests from Step {N} of a test implementation plan.

        **IMPORTANT**: This is attempt {X} to fix these tests. You MUST fix them.

        ## Failing Test Details

        **Test File(s)**:
        {List of test files with failures}

        **Failing Tests**:
        {List of specific test names that failed}

        **Error Output**:
        ```
        {Full error output from test run}
        ```

        **Original Step Context**:
        - Step: {N}/{Total} - {Step Title}
        - Test Type: {unit|component|integration|e2e}
        - What was implemented: {Brief description}

        ## Instructions

        1. **Analyze the Failures**: Understand WHY each test is failing
        2. **Read Source Files**: If needed, read the source code being tested
        3. **Fix the Tests**:
           - Edit the test files to fix the failures
           - Do NOT delete tests to make them pass - fix the actual issue
           - If the test expectation is wrong, fix the expectation
           - If the test setup is wrong, fix the setup
           - If the source code has a bug, note it but fix the test to match current behavior
        4. **Run Validation**:
           - Run: npm run lint:fix && npm run typecheck
           - Run: npm run test -- --run {test-file-path}
        5. **Return Results**:

        ## FIX RESULTS

        **Status**: success | failure
        **Attempt**: {X}

        **Changes Made**:
        - {Description of each change}

        **Root Cause**:
        - {Why the tests were failing}

        **Validation Results**:
        - Lint/Typecheck: PASS | FAIL
        - Tests: X passed, Y failed

        **Remaining Issues** (if any):
        - {List any tests still failing}
        ```

     4. Process fix subagent results
     5. Run test command AGAIN to verify independently
     6. IF tests pass: Exit loop, proceed to step logging
     7. IF tests still fail AND attempt < 3: Continue loop
     8. IF tests still fail AND attempt >= 3:
        - Log all attempts and failures
        - Use AskUserQuestion:
          - "Tests for Step {N} have failed after 3 fix attempts. How to proceed?"
          - Options:
            - "Try 3 more fix attempts"
            - "Skip this step and continue (NOT RECOMMENDED)"
            - "Abort implementation"
            - "I'll fix manually, then continue"
        - Handle user response accordingly
   END WHILE
   ````

   **Fix Loop Logging**:
   - Each fix attempt is logged in the step results file
   - Include: attempt number, error details, changes made, outcome

10. **Step Logging** (Orchestrator):
    - Create `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/0{N+2}-step-{N}-results.md`:
      - Step metadata (number, title, test type, timestamp, duration)
      - Specialist used and skills loaded
      - Subagent input (what was asked)
      - Subagent output (full response)
      - Files created
      - Test cases implemented
      - **Orchestrator verification results** (independent test run output)
      - **Fix attempts** (if any): number of attempts, changes made per attempt
      - Test results (passed/failed) - from ORCHESTRATOR verification
      - Any errors or warnings
11. **Update Todo Status** (Orchestrator):
    - If step succeeded (tests verified passing by orchestrator): Mark todo as "completed"
    - If step failed after all fix attempts exhausted: Keep as "in_progress" and log error
12. **Error Handling** (Orchestrator):
    - If subagent times out:
      - Log timeout error
      - Run tests independently to check current state
      - If tests pass: Proceed (subagent may have completed before timeout)
      - If tests fail: Enter fix loop (step 9a)
    - If subagent returns failure (non-test related):
      - Log detailed error information
      - If `--step-by-step` mode: Ask user how to proceed
      - Otherwise: Attempt recovery or abort based on severity
    - **NOTE**: Test failures are handled by the mandatory fix loop (step 9a), NOT here
13. **Step-by-Step Mode Check** (Orchestrator):
    - If `--step-by-step` flag present:
      - Use AskUserQuestion to ask user:
        - "Step {N} [{test-type}] completed with all tests passing. Continue to next step?"
        - Options: "Continue", "Skip next step", "Abort implementation"
      - Handle user response accordingly
    - **NOTE**: This check only happens AFTER tests are verified passing
14. **UPDATE INDEX** (Orchestrator): Append step summary to implementation index
15. **Progress Report** (Orchestrator):
    - Output concise progress: "âœ“ Completed step {N}/{Total} - {step title} [{test-type}] (X tests passed, verified by orchestrator)"

**Resume Mode**: If `--resume-from=N` flag present, skip to step N and begin execution there.

### Phase 4: Test Validation and Coverage Check

**Objective**: Run all implemented tests and verify coverage.

**Process**:

1. Record validation start time with ISO timestamp
2. **Mark Validation Todo** (Orchestrator):
   - Mark test validation todo as "in_progress"
3. **Run Full Test Suite** (Orchestrator or test-executor):
   - Run all new tests created in this plan:
     ```bash
     npm run test -- --run tests/unit/{feature}/**
     npm run test -- --run tests/components/{feature}/**
     npm run test -- --run tests/integration/{feature}/**
     npm run test:e2e -- tests/e2e/specs/{feature}/**
     ```
   - Capture all test output
   - Check pass/fail status for each test type
4. **Coverage Analysis** (if applicable):
   - If coverage is specified in quality gates:
     - Run tests with coverage: `npm run test -- --coverage`
     - Capture coverage report
     - Check against coverage thresholds
5. **Validation Results** (Orchestrator):
   - Create `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/XX-test-validation.md`:
     - Validation metadata (timestamp, duration)
     - Test results by type (unit, component, integration, e2e)
     - Passed/failed counts
     - Coverage report (if applicable)
     - List of any failing tests with details
6. **Validation Status Check** (Orchestrator):
   - If all tests pass: Mark validation todo as "completed"
   - If any tests fail:
     - Log failure details
     - Keep todo as "in_progress"
     - Provide recommendations for fixes
7. **UPDATE INDEX** (Orchestrator): Append validation summary to implementation index

### Phase 5: Test Implementation Summary and Completion

**Objective**: Generate comprehensive test implementation report and offer next steps.

**Process**:

1. Record completion time with ISO timestamp
2. **Calculate Statistics**:
   - Total execution time
   - Number of steps completed
   - Number of test files created
   - Number of test cases implemented
   - Tests by type (unit, component, integration, e2e)
   - Pass/fail rates (from orchestrator verification)
   - Total fix attempts across all steps
   - Steps that required fixes vs steps that passed first try
   - Coverage metrics (if applicable)
3. **Generate Change Summary**:
   - List all test files created with descriptions
   - Summarize test cases by category
   - List any fixtures/mocks created
4. **Review Todos**:
   - Count completed todos
   - List any incomplete todos
   - Identify any failures or blockers
5. **Create Implementation Summary**:
   - Save to `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/YY-implementation-summary.md`:
     - Complete execution metadata (start, end, duration)
     - Test plan reference
     - Execution mode used
     - Steps completed (N/Total)
     - Test files created summary
     - Test cases implemented by type
     - Test results summary (from orchestrator verification)
     - **Fix attempt summary**: total attempts, which steps required fixes
     - Coverage report (if applicable)
     - Known issues or warnings
     - Recommendations for next steps
6. **UPDATE INDEX**: Finalize implementation index with summary
7. **Git Commit Offer**:
   - If all tests pass:
     - Use AskUserQuestion to ask:
       - "Test implementation complete! All tests passing. Create a git commit?"
       - Options: "Yes, commit all changes", "No, I'll commit manually", "Show me git diff first"
     - If user chooses commit:
       - Generate descriptive commit message:

         ```
         test: Add tests for [Feature name]

         [Brief description of tests added]

         Test plan: docs/{date}/plans/{feature-name}-test-plan.md
         Test implementation log: docs/{date}/test-implementation/{feature-name}/

         Tests added:
         - Unit: X tests
         - Component: Y tests
         - Integration: Z tests
         - E2E: W tests

         All tests passing.

         ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

         Co-Authored-By: Claude <noreply@anthropic.com>
         ```

       - Use git commit process from Bash tool instructions

8. **Final Output to User**:

   ```
   ## Test Implementation Complete

   âœ“ Completed {N}/{Total} steps successfully
   âœ“ Created {X} test files
   âœ“ Implemented {Y} test cases
   âœ“ Tests: {Z} passed, {W} failed (verified by orchestrator)
   âœ“ Fix attempts required: {F} (across all steps)

   Test breakdown:
   - Unit: {A} tests
   - Component: {B} tests
   - Integration: {C} tests
   - E2E: {D} tests

   Implementation log: docs/{date}/test-implementation/{feature-name}/
   - ðŸ“„ 00-implementation-index.md - Navigation and overview
   - ðŸ“„ 01-pre-checks.md - Pre-implementation validation
   - ðŸ“„ 02-setup.md - Setup and test type routing
   - ðŸ“„ 03-step-1-results.md - Step 1 execution log [unit] (0 fix attempts)
   - ðŸ“„ 04-step-2-results.md - Step 2 execution log [component] (1 fix attempt)
   ...
   - ðŸ“„ XX-test-validation.md - Full test run results
   - ðŸ“„ YY-implementation-summary.md - Complete summary

   Execution time: X.X minutes

   [Any warnings or next steps]
   ```

## Error Recovery and Resilience

**Step Failure Handling**:

1. Capture full error details (message, stack trace, context)
2. Log error to step results file including test type
3. Attempt automatic recovery:
   - If test syntax error: Show error and suggest fix
   - If import error: Check if source file exists
   - If test failure: Show failing test output and expected vs actual
   - If file not found: Suggest creating file or checking plan
4. If recovery not possible:
   - Mark step as failed in todo
   - Continue to next step OR abort based on severity
   - Log failure reason clearly

**Test Failure Handling**:

1. Identify which tests failed
2. Show relevant test output with expected vs actual
3. Categorize as:
   - **Blocker**: Test implementation error (syntax, imports)
   - **Warning**: Test assertion failure (may indicate source bug or test bug)
4. Provide specific recommendations:
   - For assertion failures: "Review test expectation or source code logic"
   - For timeout failures: "Consider mocking slow operations"
   - For import errors: "Verify import paths match project structure"
5. Ask user if they want to:
   - Attempt automatic fixes
   - Abort and fix manually
   - Continue anyway (if non-blocking)

## Implementation Details

**Critical Requirements**:

- **ORCHESTRATOR PATTERN**: This command is a lightweight coordinator, NOT a direct implementer
- **TEST FOCUS**: Routes to specialized test agents (unit, component, integration, e2e, infrastructure)
- **SKILL AUTO-LOADING**: Specialized test agents automatically load testing-base + their specialized skill
- **SAFETY FIRST**: Never execute destructive operations without confirmation
- **SYSTEMATIC EXECUTION**: Execute steps in order, one at a time, via specialist delegation
- **VALIDATION ENFORCEMENT**: Specialists always run lint:fix, typecheck, and tests
- **TODO MANAGEMENT**: Orchestrator keeps todo list updated in real-time (ONE in_progress at a time)
- **COMPREHENSIVE LOGGING**: Orchestrator saves detailed logs including test types and results
- **ERROR RECOVERY**: Handle subagent errors gracefully with clear user guidance
- **QUALITY ASSURANCE**: Enforce test validation before completion

**MANDATORY TEST VERIFICATION** (NON-NEGOTIABLE):

- **NEVER TRUST SUBAGENT SELF-REPORTS**: The orchestrator MUST independently run tests after each step
- **BLOCKING PROGRESSION**: The orchestrator MUST NOT proceed to the next step if tests from the current step are failing
- **INDEPENDENT VERIFICATION**: Run `npm run test -- --run {file}` or `npm run test:e2e -- {file}` directly
- **FIX LOOP REQUIRED**: If tests fail, spawn a new subagent to fix them - repeat until passing
- **NO EXCEPTIONS**: Even if a subagent reports "all tests passing", the orchestrator MUST verify independently
- **STEP GATE**: Each step is a gate - tests must pass the orchestrator's verification to unlock the next step
- **AUDIT TRAIL**: Log both subagent-reported results AND orchestrator-verified results separately

**Quality Standards**:

- All test code must pass lint and typecheck
- All test files must follow project testing conventions (from testing-base + specialized skills)
- All tests must pass (or failures documented with reasons)
- All success criteria must be verified
- All validation commands must be executed
- All changes must be logged with test counts and results

**Logging Requirements**:

- **Human-Readable Format**: Use markdown with clear headers and sections
- **Complete Data Capture**: Full validation output, test results, error messages
- **Test Type Tracking**: Log which test type was implemented for each step
- **Incremental Saves**: Save logs after each step completes
- **Navigation Structure**: Index file with links to all logs
- **Timestamp Precision**: ISO format timestamps for all events
- **Test Results**: Document pass/fail counts, coverage metrics

## File Output Structure

**Test Implementation Logs**: `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/`

```
00-implementation-index.md          # Navigation, routing table, and workflow overview
01-pre-checks.md                    # Pre-implementation validation results
02-setup.md                         # Setup, test type detection, and routing
03-step-1-results.md                # Step 1 execution log [test-type]
04-step-2-results.md                # Step 2 execution log [test-type]
...
XX-test-validation.md               # Full test suite validation results
YY-implementation-summary.md        # Final summary with test breakdown
```

**Index File Structure** (`00-implementation-index.md`):

```markdown
# {Feature Name} Test Implementation

**Execution Date**: {timestamp}
**Test Plan**: [Link to test plan file]
**Execution Mode**: {full-auto|step-by-step|dry-run}
**Scope**: {all|unit|component|integration|e2e}
**Status**: {In Progress|Completed|Failed}

## Overview

- Total Steps: {N}
- Steps Completed: {X}/{N}
- Test Files Created: {Y}
- Test Cases Implemented: {Z}
- Tests Passed: {P} / Failed: {F}
- Total Fix Attempts: {A} (across all steps)
- Total Duration: {X.X} minutes

## Test Type Routing

| Step       | Test Type      | Specialist                     |
| ---------- | -------------- | ------------------------------ |
| 1. {title} | infrastructure | test-infrastructure-specialist |
| 2. {title} | unit           | unit-test-specialist           |
| 3. {title} | component      | component-test-specialist      |
| 4. {title} | integration    | integration-test-specialist    |
| 5. {title} | e2e            | e2e-test-specialist            |
| ...        | ...            | ...                            |

## Navigation

- [Pre-Implementation Checks](./01-pre-checks.md)
- [Setup and Routing](./02-setup.md)
- [Step 1: {title}](./03-step-1-results.md) [infrastructure]
- [Step 2: {title}](./04-step-2-results.md) [unit]
  ...
- [Test Validation](./XX-test-validation.md)
- [Implementation Summary](./YY-implementation-summary.md)

## Quick Status

| Step       | Test Type      | Specialist                     | Status | Tests | Fix Attempts | Duration |
| ---------- | -------------- | ------------------------------ | ------ | ----- | ------------ | -------- |
| 1. {title} | infrastructure | test-infrastructure-specialist | âœ“      | N/A   | 0            | 2.3s     |
| 2. {title} | unit           | unit-test-specialist           | âœ“      | 5/5   | 1            | 8.2s     |

...

## Summary

{Brief summary of test implementation results}
```

## Integration with Other Commands

**Workflow Chain**:

```bash
# Complete test implementation workflow
/plan-tests "feature area description"
# Review generated test plan
/implement-tests docs/2025_11_27/plans/feature-name-test-plan.md
```

**Integration with Feature Implementation**:

```bash
# Full feature development workflow
/plan-feature "Add new feature"
/implement-plan docs/2025_11_27/plans/feature-implementation-plan.md
/plan-tests "new feature"
/implement-tests docs/2025_11_27/plans/feature-test-plan.md
```

## Notes

- This command is designed to work seamlessly with test plans generated by `/plan-tests`
- **Architecture**: Uses orchestrator + test-specialist subagent pattern
- **Focus**: Primarily uses test-specialist for all test implementation
- Always review the test plan before executing to ensure it's current
- Use `--step-by-step` mode for complex test suites or when learning patterns
- Use `--dry-run` mode to preview test implementation steps
- Test implementation logs provide complete audit trail
- All tests are run after implementation to verify correctness
- Git commit is offered but optional - you can commit manually if preferred
