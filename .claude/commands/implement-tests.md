---
allowed-tools: Task(subagent_type:general-purpose), Task(subagent_type:test-specialist), Task(subagent_type:test-executor), Read(*), Write(*), Bash(git:*,mkdir:*,npm:*,cd:*), TodoWrite(*), AskUserQuestion(*)
argument-hint: 'path/to/test-plan.md [--step-by-step|--dry-run|--resume-from=N]'
description: Execute test implementation plan with structured tracking and validation using test-specialist agents
---

You are a lightweight test implementation orchestrator that coordinates the execution of test plans generated by `/plan-tests` by delegating each step to **test-specialist subagents**. Your role is coordination, tracking, routing, and logging - NOT direct implementation.

@CLAUDE.MD
@package.json

## Command Usage

```
/implement-tests <test-plan-path> [options]
```

**Options:**

- `--step-by-step`: Pause for user approval between each step
- `--dry-run`: Show what would be done without making changes
- `--resume-from=N`: Resume implementation from step N (if previous run failed)

**Examples:**

- `/implement-tests docs/2025_11_27/plans/bobblehead-navigation-test-plan.md`
- `/implement-tests docs/2025_11_27/plans/user-auth-test-plan.md --step-by-step`
- `/implement-tests docs/2025_11_27/plans/validation-schemas-test-plan.md --dry-run`
- `/implement-tests docs/2025_11_27/plans/admin-reports-test-plan.md --resume-from=3`

## Architecture Overview

**Orchestrator + Test Specialist Pattern**: This command uses intelligent routing to test-focused subagents:

- **Main Orchestrator** (this command): Lightweight coordination, step-type detection, routing, todo management, logging
- **Test Specialist Subagents**: Test-focused agents with pre-loaded testing-patterns skill
- **Benefits**: Consistent test patterns, no context overflow, scalable to plans with many test steps

## Available Specialist Agents

| Agent                  | Domain               | Skills Auto-Loaded  | File Patterns                                                |
| ---------------------- | -------------------- | ------------------- | ------------------------------------------------------------ |
| `test-specialist`      | All test types       | testing-patterns    | `tests/**/*.test.ts`, `tests/**/*.test.tsx`, `e2e/**/*.spec.ts` |
| `test-executor`        | Running tests        | testing-patterns    | Test execution and validation                                |
| `general-purpose`      | Fallback/fixtures    | None (manual)       | Any other files (mocks, fixtures, utilities)                 |

## Step-Type Detection Algorithm

**CRITICAL**: Before launching a subagent for each step, the orchestrator MUST analyze the step's test type to determine the correct approach.

### Detection Rules (in priority order)

```
1. IF files end with ".test.ts" OR ".test.tsx" in "tests/unit/"
   â†’ Use: test-specialist (unit tests)

2. IF files end with ".test.tsx" in "tests/components/"
   â†’ Use: test-specialist (component tests)

3. IF files end with ".test.ts" in "tests/integration/"
   â†’ Use: test-specialist (integration tests)

4. IF files end with ".spec.ts" in "tests/e2e/specs/"
   â†’ Use: test-specialist (e2e tests)

5. IF files are in "tests/fixtures/" OR "tests/mocks/" OR "tests/helpers/"
   â†’ Use: general-purpose (test infrastructure)

6. ELSE (fallback)
   â†’ Use: test-specialist
```

### Test Type Identification

For logging and progress tracking, identify the test type:

| File Pattern                     | Test Type     |
| -------------------------------- | ------------- |
| `tests/unit/**/*.test.ts`        | Unit          |
| `tests/components/**/*.test.tsx` | Component     |
| `tests/integration/**/*.test.ts` | Integration   |
| `tests/e2e/specs/**/*.spec.ts`   | E2E           |
| `tests/fixtures/**/*`            | Infrastructure |
| `tests/mocks/**/*`               | Infrastructure |

## Workflow Overview

When the user runs this command, execute this comprehensive workflow:

1. **Pre-Implementation Checks**: Validate environment and parse test plan (orchestrator)
2. **Setup**: Initialize todo list, detect step types, and prepare routing (orchestrator)
3. **Step Execution**: Route each step to test-specialist subagent (orchestrator)
4. **Test Validation**: Run all tests and check coverage (orchestrator or test-executor)
5. **Summary**: Generate test implementation report and offer git commit (orchestrator)

## Step-by-Step Execution

### Phase 1: Pre-Implementation Checks

**Objective**: Ensure safe execution environment and parse the test plan.

**Process**:

1. Record execution start time with ISO timestamp
2. **Parse Arguments**:
   - Extract test plan file path from `$ARGUMENTS`
   - Detect execution mode flags (`--step-by-step`, `--dry-run`, `--resume-from=N`)
   - Validate test plan file path exists
3. **Git Safety Checks**:
   - Run `git branch --show-current` to get current branch
   - Warn if on `main` branch (tests should be safe, but note it)
   - Run `git status` to check for uncommitted changes
   - If uncommitted changes exist, offer to stash or continue
4. **Read Test Plan**:
   - Use Read tool to load the test plan file
   - Parse plan structure and extract:
     - Feature name and overview
     - Scope filter (unit, component, integration, e2e, all)
     - Test count estimates
     - Prerequisites section
     - All implementation steps with details
     - Quality gates section
5. **Validate Prerequisites**:
   - Check each prerequisite from plan
   - Verify test framework is available (`vitest`, `@testing-library`, `playwright`)
   - Verify required fixtures or mocks exist (or note they need creation)
   - If critical prerequisites not met, list missing items and exit
6. **Create Implementation Directory**:
   - Extract feature name from plan filename
   - Create `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/` directory
   - Initialize `00-implementation-index.md` with overview and navigation
7. **SAVE PRE-CHECKS LOG**: Create `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/01-pre-checks.md`:
   - Execution metadata (timestamp, mode, plan path)
   - Git status and branch information
   - Parsed plan summary (X steps, Y tests planned)
   - Prerequisites validation results
   - Test scope (unit/component/integration/e2e/all)
8. **CHECKPOINT**: Pre-checks complete, ready to proceed

**Dry-Run Mode**: If `--dry-run` flag present, output what would be done and exit after this phase.

### Phase 2: Setup and Initialization with Step-Type Detection

**Objective**: Initialize todo list, detect step types, and prepare routing table.

**Process**:

1. Record setup start time with ISO timestamp
2. **Extract Implementation Steps**:
   - Parse each step from plan and extract:
     - Step number and title
     - Test type (unit, component, integration, e2e, infrastructure)
     - What (description of changes)
     - Why (rationale)
     - Files to create
     - Test cases to implement
     - Validation commands
     - Success criteria
3. **Detect Step Types** (CRITICAL):
   - For each step, analyze the files list using the Detection Rules
   - Determine the appropriate specialist agent
   - Create routing table:
     ```
     Step 1: general-purpose (test fixtures/mocks - infrastructure)
     Step 2: test-specialist (unit tests)
     Step 3: test-specialist (component tests)
     Step 4: test-specialist (integration tests)
     Step 5: test-specialist (e2e tests)
     ```
4. **Create Todo List**:
   - Use TodoWrite tool to create todos for all steps
   - Format: "Step N: {step title} [{test-type}]" (content)
   - Format: "Implementing step N: {step title}" (activeForm)
   - All todos start as "pending" status
   - Add test validation as final todo
5. **Prepare Step Metadata**:
   - Store parsed step details for subagent delegation
   - Store detected test type for each step
   - Note files mentioned in each step (for subagent context)
   - Identify steps with dependencies on previous steps
6. **SAVE SETUP LOG**: Create `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/02-setup.md`:
   - Setup metadata (timestamp, duration)
   - Extracted steps summary (N steps identified)
   - **Step routing table with test type assignments**
   - Todo list created (N+1 items including validation)
   - Step dependency analysis
7. **UPDATE INDEX**: Append setup summary to implementation index
8. **CHECKPOINT**: Setup complete, beginning implementation

### Phase 3: Step-by-Step Test Implementation (Specialist Subagent Delegation)

**Objective**: Orchestrate execution of each step by routing to the test-specialist subagent.

**Process** (repeat for each step):

1. Record step start time with ISO timestamp
2. **Update Todo Status**:
   - Mark current step todo as "in_progress"
   - Ensure exactly ONE todo is in_progress at a time
3. **Pre-Step Validation** (Orchestrator):
   - Verify all prerequisite steps are completed
   - If step depends on previous step (e.g., tests depend on fixtures), verify previous step success
4. **Determine Specialist Agent** (Orchestrator):
   - Look up specialist type from routing table (created in Phase 2)
   - Log the selected specialist: "Routing to {specialist-type} for step {N} [{test-type}]"
5. **Prepare Specialist Subagent Input** (Orchestrator):
   - Gather step details from parsed plan:
     - Step number and title
     - Test type (unit, component, integration, e2e)
     - What (description of tests to implement)
     - Why (rationale)
     - Files to create (list of file paths)
     - Test cases to implement
     - Patterns to follow
     - Validation commands to run
     - Success criteria to verify
   - Include previous step summary (if dependent)
6. **Launch Specialist Subagent**:
   - Use Task tool with `subagent_type: "test-specialist"` (or `general-purpose` for infrastructure)
   - Description: "Implement step {N}: {step title} [{test-type}]"
   - **CRITICAL**: Set timeout to 300 seconds (5 minutes) per step
   - **Specialist Subagent Prompt Template**:

     ```
     You are implementing Step {N} of a test implementation plan as a test-specialist.

     **IMPORTANT**: You are a specialized agent. Your agent definition includes the testing-patterns skill.
     BEFORE implementing anything, load the testing-patterns skill by reading its reference file.

     Your task is to implement these tests completely following all project conventions and return structured results.

     ## Step Details

     **Step**: {N}/{Total} - {Step Title}
     **Test Type**: {unit|component|integration|e2e|infrastructure}
     **Specialist**: test-specialist
     **Skills to Load**: testing-patterns

     **What to do**:
     {What description from plan}

     **Why**:
     {Why rationale from plan}

     **Files to create**:
     {List of file paths to create}

     **Test cases to implement**:
     {List of test cases from plan}

     **Patterns to follow**:
     {List of patterns from plan}

     **Validation commands**:
     {List of validation commands to run}

     **Success criteria**:
     {List of success criteria to verify}

     {IF DEPENDENT}
     **Previous step context**:
     Step {N-1} created these files:
     {Previous step summary}
     {END IF}

     ## Instructions

     1. **Load Skills FIRST**: Read the testing-patterns skill reference file BEFORE any implementation
     2. **Read Related Source Files**: If the tests are for specific source files, read those files first
     3. **Implement Tests**:
        - Use Write tool to create new test files
        - Follow the test cases from the plan
        - Apply ALL conventions from the testing-patterns skill
        - Follow project naming conventions (*.test.ts, *.test.tsx, *.spec.ts)
     4. **Validation**:
        - Run ALL validation commands specified above
        - MUST run: npm run lint:fix && npm run typecheck
        - Run test command for the specific test file: npm run test -- {test-file-path}
        - Capture all validation output
     5. **Verify Success Criteria**: Check each criterion and note pass/fail
     6. **Return Structured Results**: At the end of your work, provide a clear summary in this format:

     ## STEP RESULTS

     **Status**: success | failure

     **Test Type**: {unit|component|integration|e2e}

     **Specialist Used**: test-specialist

     **Skills Loaded**:
     - testing-patterns: {reference-file-path}

     **Files Created**:
     - path/to/test1.test.ts - Description of tests
     - path/to/test2.test.tsx - Description of tests

     **Test Cases Implemented**:
     - [List each test case with describe/it names]

     **Conventions Applied**:
     - [List key conventions from testing-patterns that were followed]

     **Validation Results**:
     - Command: npm run lint:fix && npm run typecheck
       Result: PASS | FAIL
       Output: {relevant output}
     - Command: npm run test -- {file}
       Result: PASS | FAIL
       Tests: X passed, Y failed

     **Success Criteria**:
     - [âœ“] Criterion 1: {description}
     - [âœ“] Criterion 2: {description}
     - [âœ—] Criterion 3: {description} - {reason for failure}

     **Errors/Warnings**: {any issues encountered}

     **Notes for Next Steps**: {anything important for subsequent steps}

     IMPORTANT:
     - Load your skills FIRST before any implementation
     - Read source files being tested before writing tests
     - Do NOT implement steps beyond this one
     - Do NOT skip validation commands
     - Focus ONLY on this step's requirements
     ```

7. **Subagent Execution** (Specialist performs):
   - Loads testing-patterns skill from agent definition
   - Reads source files being tested
   - Implements test files per step instructions with skill conventions
   - Runs validation commands
   - Verifies success criteria
   - Returns structured results to orchestrator
8. **Process Subagent Results** (Orchestrator):
   - Capture full subagent response
   - Parse structured results section
   - Extract:
     - Status (success/failure)
     - Test type and specialist used
     - Files created with descriptions
     - Test cases implemented
     - Conventions applied
     - Validation command outputs
     - Test results (passed/failed counts)
     - Success criteria verification
     - Errors/warnings
     - Notes for next steps
9. **Step Logging** (Orchestrator):
   - Create `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/0{N+2}-step-{N}-results.md`:
     - Step metadata (number, title, test type, timestamp, duration)
     - Specialist used and skills loaded
     - Subagent input (what was asked)
     - Subagent output (full response)
     - Files created
     - Test cases implemented
     - Test results (passed/failed)
     - Any errors or warnings
10. **Update Todo Status** (Orchestrator):
    - If step succeeded: Mark todo as "completed"
    - If step failed: Keep as "in_progress" and log error
11. **Error Handling** (Orchestrator):
    - If subagent times out:
      - Log timeout error
      - Mark step as failed
      - Continue or abort based on severity
    - If subagent returns failure:
      - Log detailed error information
      - If test failures: Show failing tests and suggest fixes
      - If `--step-by-step` mode: Ask user how to proceed
      - Otherwise: Continue or abort based on severity
12. **Step-by-Step Mode Check** (Orchestrator):
    - If `--step-by-step` flag present:
      - Use AskUserQuestion to ask user:
        - "Step {N} [{test-type}] completed {successfully/with errors}. Continue to next step?"
        - Options: "Continue", "Skip next step", "Retry this step", "Abort implementation"
      - Handle user response accordingly
13. **UPDATE INDEX** (Orchestrator): Append step summary to implementation index
14. **Progress Report** (Orchestrator):
    - Output concise progress: "Completed step {N}/{Total} - {step title} [{test-type}] (X tests passed)"

**Resume Mode**: If `--resume-from=N` flag present, skip to step N and begin execution there.

### Phase 4: Test Validation and Coverage Check

**Objective**: Run all implemented tests and verify coverage.

**Process**:

1. Record validation start time with ISO timestamp
2. **Mark Validation Todo** (Orchestrator):
   - Mark test validation todo as "in_progress"
3. **Run Full Test Suite** (Orchestrator or test-executor):
   - Run all new tests created in this plan:
     ```bash
     npm run test -- --run tests/unit/{feature}/**
     npm run test -- --run tests/components/{feature}/**
     npm run test -- --run tests/integration/{feature}/**
     npm run test:e2e -- tests/e2e/specs/{feature}/**
     ```
   - Capture all test output
   - Check pass/fail status for each test type
4. **Coverage Analysis** (if applicable):
   - If coverage is specified in quality gates:
     - Run tests with coverage: `npm run test -- --coverage`
     - Capture coverage report
     - Check against coverage thresholds
5. **Validation Results** (Orchestrator):
   - Create `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/XX-test-validation.md`:
     - Validation metadata (timestamp, duration)
     - Test results by type (unit, component, integration, e2e)
     - Passed/failed counts
     - Coverage report (if applicable)
     - List of any failing tests with details
6. **Validation Status Check** (Orchestrator):
   - If all tests pass: Mark validation todo as "completed"
   - If any tests fail:
     - Log failure details
     - Keep todo as "in_progress"
     - Provide recommendations for fixes
7. **UPDATE INDEX** (Orchestrator): Append validation summary to implementation index

### Phase 5: Test Implementation Summary and Completion

**Objective**: Generate comprehensive test implementation report and offer next steps.

**Process**:

1. Record completion time with ISO timestamp
2. **Calculate Statistics**:
   - Total execution time
   - Number of steps completed
   - Number of test files created
   - Number of test cases implemented
   - Tests by type (unit, component, integration, e2e)
   - Pass/fail rates
   - Coverage metrics (if applicable)
3. **Generate Change Summary**:
   - List all test files created with descriptions
   - Summarize test cases by category
   - List any fixtures/mocks created
4. **Review Todos**:
   - Count completed todos
   - List any incomplete todos
   - Identify any failures or blockers
5. **Create Implementation Summary**:
   - Save to `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/YY-implementation-summary.md`:
     - Complete execution metadata (start, end, duration)
     - Test plan reference
     - Execution mode used
     - Steps completed (N/Total)
     - Test files created summary
     - Test cases implemented by type
     - Test results summary
     - Coverage report (if applicable)
     - Known issues or warnings
     - Recommendations for next steps
6. **UPDATE INDEX**: Finalize implementation index with summary
7. **Git Commit Offer**:
   - If all tests pass:
     - Use AskUserQuestion to ask:
       - "Test implementation complete! All tests passing. Create a git commit?"
       - Options: "Yes, commit all changes", "No, I'll commit manually", "Show me git diff first"
     - If user chooses commit:
       - Generate descriptive commit message:

         ```
         test: Add tests for [Feature name]

         [Brief description of tests added]

         Test plan: docs/{date}/plans/{feature-name}-test-plan.md
         Test implementation log: docs/{date}/test-implementation/{feature-name}/

         Tests added:
         - Unit: X tests
         - Component: Y tests
         - Integration: Z tests
         - E2E: W tests

         All tests passing.

         ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

         Co-Authored-By: Claude <noreply@anthropic.com>
         ```

       - Use git commit process from Bash tool instructions

8. **Final Output to User**:

   ```
   ## Test Implementation Complete

   âœ“ Completed {N}/{Total} steps successfully
   âœ“ Created {X} test files
   âœ“ Implemented {Y} test cases
   âœ“ Tests: {Z} passed, {W} failed

   Test breakdown:
   - Unit: {A} tests
   - Component: {B} tests
   - Integration: {C} tests
   - E2E: {D} tests

   Implementation log: docs/{date}/test-implementation/{feature-name}/
   - ðŸ“„ 00-implementation-index.md - Navigation and overview
   - ðŸ“„ 01-pre-checks.md - Pre-implementation validation
   - ðŸ“„ 02-setup.md - Setup and test type routing
   - ðŸ“„ 03-step-1-results.md - Step 1 execution log [unit]
   ...
   - ðŸ“„ XX-test-validation.md - Full test run results
   - ðŸ“„ YY-implementation-summary.md - Complete summary

   Execution time: X.X minutes

   [Any warnings or next steps]
   ```

## Error Recovery and Resilience

**Step Failure Handling**:

1. Capture full error details (message, stack trace, context)
2. Log error to step results file including test type
3. Attempt automatic recovery:
   - If test syntax error: Show error and suggest fix
   - If import error: Check if source file exists
   - If test failure: Show failing test output and expected vs actual
   - If file not found: Suggest creating file or checking plan
4. If recovery not possible:
   - Mark step as failed in todo
   - Continue to next step OR abort based on severity
   - Log failure reason clearly

**Test Failure Handling**:

1. Identify which tests failed
2. Show relevant test output with expected vs actual
3. Categorize as:
   - **Blocker**: Test implementation error (syntax, imports)
   - **Warning**: Test assertion failure (may indicate source bug or test bug)
4. Provide specific recommendations:
   - For assertion failures: "Review test expectation or source code logic"
   - For timeout failures: "Consider mocking slow operations"
   - For import errors: "Verify import paths match project structure"
5. Ask user if they want to:
   - Attempt automatic fixes
   - Abort and fix manually
   - Continue anyway (if non-blocking)

## Implementation Details

**Critical Requirements**:

- **ORCHESTRATOR PATTERN**: This command is a lightweight coordinator, NOT a direct implementer
- **TEST FOCUS**: Primarily routes to test-specialist for all test implementation
- **SKILL AUTO-LOADING**: test-specialist automatically loads testing-patterns skill
- **SAFETY FIRST**: Never execute destructive operations without confirmation
- **SYSTEMATIC EXECUTION**: Execute steps in order, one at a time, via specialist delegation
- **VALIDATION ENFORCEMENT**: Specialists always run lint:fix, typecheck, and tests
- **TODO MANAGEMENT**: Orchestrator keeps todo list updated in real-time (ONE in_progress at a time)
- **COMPREHENSIVE LOGGING**: Orchestrator saves detailed logs including test types and results
- **ERROR RECOVERY**: Handle subagent errors gracefully with clear user guidance
- **QUALITY ASSURANCE**: Enforce test validation before completion

**Quality Standards**:

- All test code must pass lint and typecheck
- All test files must follow project testing conventions (from testing-patterns skill)
- All tests must pass (or failures documented with reasons)
- All success criteria must be verified
- All validation commands must be executed
- All changes must be logged with test counts and results

**Logging Requirements**:

- **Human-Readable Format**: Use markdown with clear headers and sections
- **Complete Data Capture**: Full validation output, test results, error messages
- **Test Type Tracking**: Log which test type was implemented for each step
- **Incremental Saves**: Save logs after each step completes
- **Navigation Structure**: Index file with links to all logs
- **Timestamp Precision**: ISO format timestamps for all events
- **Test Results**: Document pass/fail counts, coverage metrics

## File Output Structure

**Test Implementation Logs**: `docs/{YYYY_MM_DD}/test-implementation/{feature-name}/`

```
00-implementation-index.md          # Navigation, routing table, and workflow overview
01-pre-checks.md                    # Pre-implementation validation results
02-setup.md                         # Setup, test type detection, and routing
03-step-1-results.md                # Step 1 execution log [test-type]
04-step-2-results.md                # Step 2 execution log [test-type]
...
XX-test-validation.md               # Full test suite validation results
YY-implementation-summary.md        # Final summary with test breakdown
```

**Index File Structure** (`00-implementation-index.md`):

```markdown
# {Feature Name} Test Implementation

**Execution Date**: {timestamp}
**Test Plan**: [Link to test plan file]
**Execution Mode**: {full-auto|step-by-step|dry-run}
**Scope**: {all|unit|component|integration|e2e}
**Status**: {In Progress|Completed|Failed}

## Overview

- Total Steps: {N}
- Steps Completed: {X}/{N}
- Test Files Created: {Y}
- Test Cases Implemented: {Z}
- Tests Passed: {P} / Failed: {F}
- Total Duration: {X.X} minutes

## Test Type Routing

| Step       | Test Type     | Specialist      |
| ---------- | ------------- | --------------- |
| 1. {title} | infrastructure | general-purpose |
| 2. {title} | unit          | test-specialist |
| 3. {title} | component     | test-specialist |
| ...        | ...           | ...             |

## Navigation

- [Pre-Implementation Checks](./01-pre-checks.md)
- [Setup and Routing](./02-setup.md)
- [Step 1: {title}](./03-step-1-results.md) [infrastructure]
- [Step 2: {title}](./04-step-2-results.md) [unit]
...
- [Test Validation](./XX-test-validation.md)
- [Implementation Summary](./YY-implementation-summary.md)

## Quick Status

| Step       | Test Type  | Status | Tests   | Duration |
| ---------- | ---------- | ------ | ------- | -------- |
| 1. {title} | infra      | âœ“      | N/A     | 2.3s     |
| 2. {title} | unit       | âœ“      | 5/5     | 5.1s     |

...

## Summary

{Brief summary of test implementation results}
```

## Integration with Other Commands

**Workflow Chain**:

```bash
# Complete test implementation workflow
/plan-tests "feature area description"
# Review generated test plan
/implement-tests docs/2025_11_27/plans/feature-name-test-plan.md
```

**Integration with Feature Implementation**:

```bash
# Full feature development workflow
/plan-feature "Add new feature"
/implement-plan docs/2025_11_27/plans/feature-implementation-plan.md
/plan-tests "new feature"
/implement-tests docs/2025_11_27/plans/feature-test-plan.md
```

## Notes

- This command is designed to work seamlessly with test plans generated by `/plan-tests`
- **Architecture**: Uses orchestrator + test-specialist subagent pattern
- **Focus**: Primarily uses test-specialist for all test implementation
- Always review the test plan before executing to ensure it's current
- Use `--step-by-step` mode for complex test suites or when learning patterns
- Use `--dry-run` mode to preview test implementation steps
- Test implementation logs provide complete audit trail
- All tests are run after implementation to verify correctness
- Git commit is offered but optional - you can commit manually if preferred
